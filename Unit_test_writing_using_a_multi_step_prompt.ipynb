{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/ML-AI-cookbook/blob/main/Unit_test_writing_using_a_multi_step_prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwwMW1PQmwho"
      },
      "source": [
        "# Unit test writing using a multi-step prompt\n",
        "\n",
        "Complex tasks, such as writing unit tests, can benefit from multi-step prompts. In contrast to a single prompt, a multi-step prompt generates text from GPT and then feeds that output text back into subsequent prompts. This can help in cases where you want GPT to reason things out before answering, or brainstorm a plan before executing it.\n",
        "\n",
        "In this notebook, we use a 3-step prompt to write unit tests in Python using the following steps:\n",
        "\n",
        "1. **Explain**: Given a Python function, we ask GPT to explain what the function is doing and why.\n",
        "2. **Plan**: We ask GPT to plan a set of unit tests for the function.\n",
        "    - If the plan is too short, we ask GPT to elaborate with more ideas for unit tests.\n",
        "3. **Execute**: Finally, we instruct GPT to write unit tests that cover the planned cases.\n",
        "\n",
        "The code example illustrates a few embellishments on the chained, multi-step prompt:\n",
        "\n",
        "- Conditional branching (e.g., asking for elaboration only if the first plan is too short)\n",
        "- The choice of different models for different steps\n",
        "- A check that re-runs the function if the output is unsatisfactory (e.g., if the output code cannot be parsed by Python's `ast` module)\n",
        "- Streaming output so that you can start reading the output before it's fully generated (handy for long, multi-step outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-lgo-ftmyil",
        "outputId": "99477579-2040-40db-f3c9-468600f83c6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m61.4/76.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rO8O_dylmwhr"
      },
      "outputs": [],
      "source": [
        "# imports needed to run the code in this notebook\n",
        "import ast  # used for detecting whether generated Python code is valid\n",
        "import openai  # used for calling the OpenAI API\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "\n",
        "color_prefix_by_role = {\n",
        "    \"system\": \"\\033[0m\",  # gray\n",
        "    \"user\": \"\\033[0m\",  # gray\n",
        "    \"assistant\": \"\\033[92m\",  # green\n",
        "}\n",
        "\n",
        "\n",
        "def print_messages(messages, color_prefix_by_role=color_prefix_by_role) -> None:\n",
        "    \"\"\"Prints messages sent to or from GPT.\"\"\"\n",
        "    for message in messages:\n",
        "        role = message[\"role\"]\n",
        "        color_prefix = color_prefix_by_role[role]\n",
        "        content = message[\"content\"]\n",
        "        print(f\"{color_prefix}\\n[{role}]\\n{content}\")\n",
        "\n",
        "\n",
        "def print_message_delta(delta, color_prefix_by_role=color_prefix_by_role) -> None:\n",
        "    \"\"\"Prints a chunk of messages streamed back from GPT.\"\"\"\n",
        "    if \"role\" in delta:\n",
        "        role = delta[\"role\"]\n",
        "        color_prefix = color_prefix_by_role[role]\n",
        "        print(f\"{color_prefix}\\n[{role}]\\n\", end=\"\")\n",
        "    elif \"content\" in delta:\n",
        "        content = delta[\"content\"]\n",
        "        print(content, end=\"\")\n",
        "    else:\n",
        "        pass\n",
        "\n",
        "\n",
        "# example of a function that uses a multi-step prompt to write unit tests\n",
        "def unit_tests_from_function(\n",
        "    function_to_test: str,  # Python function to test, as a string\n",
        "    unit_test_package: str = \"pytest\",  # unit testing package; use the name as it appears in the import statement\n",
        "    approx_min_cases_to_cover: int = 5,  # minimum number of test case categories to cover (approximate)\n",
        "    print_text: bool = False,  # optionally prints text; helpful for understanding the function & debugging\n",
        "    explain_model: str = \"gpt-3.5-turbo\",  # model used to generate text plans in step 1\n",
        "    plan_model: str = \"gpt-3.5-turbo\",  # model used to generate text plans in steps 2 and 2b\n",
        "    execute_model: str = \"gpt-3.5-turbo\",  # model used to generate code in step 3\n",
        "    temperature: float = 0.4,  # temperature = 0 can sometimes get stuck in repetitive loops, so we use 0.4\n",
        "    reruns_if_fail: int = 1,  # if the output code cannot be parsed, this will re-run the function up to N times\n",
        ") -> str:\n",
        "    \"\"\"Returns a unit test for a given Python function, using a 3-step GPT prompt.\"\"\"\n",
        "\n",
        "    # Step 1: Generate an explanation of the function\n",
        "\n",
        "    # create a markdown-formatted message that asks GPT to explain the function, formatted as a bullet list\n",
        "    explain_system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\",\n",
        "    }\n",
        "    explain_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
        "\n",
        "```python\n",
        "{function_to_test}\n",
        "```\"\"\",\n",
        "    }\n",
        "    explain_messages = [explain_system_message, explain_user_message]\n",
        "    if print_text:\n",
        "        print_messages(explain_messages)\n",
        "\n",
        "    explanation_response = openai.ChatCompletion.create(\n",
        "        model=explain_model,\n",
        "        messages=explain_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "    explanation = \"\"\n",
        "    for chunk in explanation_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            explanation += delta[\"content\"]\n",
        "    explain_assistant_message = {\"role\": \"assistant\", \"content\": explanation}\n",
        "\n",
        "    # Step 2: Generate a plan to write a unit test\n",
        "\n",
        "    # Asks GPT to plan out cases the units tests should cover, formatted as a bullet list\n",
        "    plan_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"A good unit test suite should aim to:\n",
        "- Test the function's behavior for a wide range of possible inputs\n",
        "- Test edge cases that the author may not have foreseen\n",
        "- Take advantage of the features of `{unit_test_package}` to make the tests easy to write and maintain\n",
        "- Be easy to read and understand, with clean code and descriptive names\n",
        "- Be deterministic, so that the tests always pass or fail in the same way\n",
        "\n",
        "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\"\"\",\n",
        "    }\n",
        "    plan_messages = [\n",
        "        explain_system_message,\n",
        "        explain_user_message,\n",
        "        explain_assistant_message,\n",
        "        plan_user_message,\n",
        "    ]\n",
        "    if print_text:\n",
        "        print_messages([plan_user_message])\n",
        "    plan_response = openai.ChatCompletion.create(\n",
        "        model=plan_model,\n",
        "        messages=plan_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "    plan = \"\"\n",
        "    for chunk in plan_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            plan += delta[\"content\"]\n",
        "    plan_assistant_message = {\"role\": \"assistant\", \"content\": plan}\n",
        "\n",
        "    # Step 2b: If the plan is short, ask GPT to elaborate further\n",
        "    # this counts top-level bullets (e.g., categories), but not sub-bullets (e.g., test cases)\n",
        "    num_bullets = max(plan.count(\"\\n-\"), plan.count(\"\\n*\"))\n",
        "    elaboration_needed = num_bullets < approx_min_cases_to_cover\n",
        "    if elaboration_needed:\n",
        "        elaboration_user_message = {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\"\"\",\n",
        "        }\n",
        "        elaboration_messages = [\n",
        "            explain_system_message,\n",
        "            explain_user_message,\n",
        "            explain_assistant_message,\n",
        "            plan_user_message,\n",
        "            plan_assistant_message,\n",
        "            elaboration_user_message,\n",
        "        ]\n",
        "        if print_text:\n",
        "            print_messages([elaboration_user_message])\n",
        "        elaboration_response = openai.ChatCompletion.create(\n",
        "            model=plan_model,\n",
        "            messages=elaboration_messages,\n",
        "            temperature=temperature,\n",
        "            stream=True,\n",
        "        )\n",
        "        elaboration = \"\"\n",
        "        for chunk in elaboration_response:\n",
        "            delta = chunk[\"choices\"][0][\"delta\"]\n",
        "            if print_text:\n",
        "                print_message_delta(delta)\n",
        "            if \"content\" in delta:\n",
        "                elaboration += delta[\"content\"]\n",
        "        elaboration_assistant_message = {\"role\": \"assistant\", \"content\": elaboration}\n",
        "\n",
        "    # Step 3: Generate the unit test\n",
        "\n",
        "    # create a markdown-formatted prompt that asks GPT to complete a unit test\n",
        "    package_comment = \"\"\n",
        "    if unit_test_package == \"pytest\":\n",
        "        package_comment = \"# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\"\n",
        "    execute_system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\",\n",
        "    }\n",
        "    execute_user_message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Using Python and the `{unit_test_package}` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
        "\n",
        "```python\n",
        "# imports\n",
        "import {unit_test_package}  # used for our unit tests\n",
        "{{insert other imports as needed}}\n",
        "\n",
        "# function to test\n",
        "{function_to_test}\n",
        "\n",
        "# unit tests\n",
        "{package_comment}\n",
        "{{insert unit test code here}}\n",
        "```\"\"\",\n",
        "    }\n",
        "    execute_messages = [\n",
        "        execute_system_message,\n",
        "        explain_user_message,\n",
        "        explain_assistant_message,\n",
        "        plan_user_message,\n",
        "        plan_assistant_message,\n",
        "    ]\n",
        "    if elaboration_needed:\n",
        "        execute_messages += [elaboration_user_message, elaboration_assistant_message]\n",
        "    execute_messages += [execute_user_message]\n",
        "    if print_text:\n",
        "        print_messages([execute_system_message, execute_user_message])\n",
        "\n",
        "    execute_response = openai.ChatCompletion.create(\n",
        "        model=execute_model,\n",
        "        messages=execute_messages,\n",
        "        temperature=temperature,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    print(execute_response)\n",
        "\n",
        "    execution = \"\"\n",
        "    for chunk in execute_response:\n",
        "        delta = chunk[\"choices\"][0][\"delta\"]\n",
        "        if print_text:\n",
        "            print_message_delta(delta)\n",
        "        if \"content\" in delta:\n",
        "            execution += delta[\"content\"]\n",
        "\n",
        "    # check the output for errors\n",
        "    code = execution.split(\"```python\")[1].split(\"```\")[0].strip()\n",
        "    try:\n",
        "        ast.parse(code)\n",
        "    except SyntaxError as e:\n",
        "        print(f\"Syntax error in generated code: {e}\")\n",
        "        if reruns_if_fail > 0:\n",
        "            print(\"Rerunning...\")\n",
        "            return unit_tests_from_function(\n",
        "                function_to_test=function_to_test,\n",
        "                unit_test_package=unit_test_package,\n",
        "                approx_min_cases_to_cover=approx_min_cases_to_cover,\n",
        "                print_text=print_text,\n",
        "                explain_model=explain_model,\n",
        "                plan_model=plan_model,\n",
        "                execute_model=execute_model,\n",
        "                temperature=temperature,\n",
        "                reruns_if_fail=reruns_if_fail\n",
        "                - 1,  # decrement rerun counter when calling again\n",
        "            )\n",
        "\n",
        "    # return the unit test as a string\n",
        "    return code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3tzETYKmwhu",
        "outputId": "a5cae602-a4ec-4fb2-f059-51d739b6ee88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\n",
            "[system]\n",
            "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You carefully explain code with great detail and accuracy. You organize your explanations in markdown-formatted, bulleted lists.\n",
            "\u001b[0m\n",
            "[user]\n",
            "Please explain the following Python function. Review what each element of the function is doing precisely and what the author's intentions may have been. Organize your explanation as a markdown-formatted, bulleted list.\n",
            "\n",
            "```python\n",
            "\n",
            "def add(a,b):\n",
            "  return a+b\n",
            "\n",
            "```\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "Here is an explanation of the provided Python function:\n",
            "\n",
            "- The function is named `add`.\n",
            "- It takes two parameters, `a` and `b`, which are expected to be numbers.\n",
            "- The function returns the sum of `a` and `b` by using the `+` operator.\n",
            "- The author's intention is to create a simple function that can be used to add two numbers together.\n",
            "- The function does not perform any type checking or validation, assuming that the inputs will always be numbers.\u001b[0m\n",
            "[user]\n",
            "A good unit test suite should aim to:\n",
            "- Test the function's behavior for a wide range of possible inputs\n",
            "- Test edge cases that the author may not have foreseen\n",
            "- Take advantage of the features of `pytest` to make the tests easy to write and maintain\n",
            "- Be easy to read and understand, with clean code and descriptive names\n",
            "- Be deterministic, so that the tests always pass or fail in the same way\n",
            "\n",
            "To help unit test the function above, list diverse scenarios that the function should be able to handle (and under each scenario, include a few examples as sub-bullets).\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "Here are some diverse scenarios that the `add` function should be able to handle:\n",
            "\n",
            "1. Basic addition:\n",
            "   - Example: `add(2, 3)` should return `5`\n",
            "   - Example: `add(0, 0)` should return `0`\n",
            "   - Example: `add(-5, 8)` should return `3`\n",
            "\n",
            "2. Addition with floating-point numbers:\n",
            "   - Example: `add(2.5, 1.75)` should return `4.25`\n",
            "   - Example: `add(0.1, 0.2)` should return `0.3`\n",
            "   - Example: `add(-3.5, 6.25)` should return `2.75`\n",
            "\n",
            "3. Addition with negative numbers:\n",
            "   - Example: `add(-2, -3)` should return `-5`\n",
            "   - Example: `add(-5, 3)` should return `-2`\n",
            "   - Example: `add(-2.5, -1.75)` should return `-4.25`\n",
            "\n",
            "4. Addition with large numbers:\n",
            "   - Example: `add(1000000, 2000000)` should return `3000000`\n",
            "   - Example: `add(999999999, 1)` should return `1000000000`\n",
            "   - Example: `add(1e100, 1e100)` should return `2e100`\n",
            "\n",
            "5. Addition with different data types:\n",
            "   - Example: `add(\"Hello\", \" World\")` should return `\"Hello World\"`\n",
            "   - Example: `add([1, 2, 3], [4, 5, 6])` should return `[1, 2, 3, 4, 5, 6]`\n",
            "   - Example: `add((1, 2), (3, 4))` should return `(1, 2, 3, 4)`\n",
            "\n",
            "6. Addition with invalid inputs:\n",
            "   - Example: `add(\"2\", 3)` should raise a `TypeError` since one of the inputs is a string\n",
            "   - Example: `add(2, \"3\")` should raise a `TypeError` since one of the inputs is a string\n",
            "   - Example: `add(\"two\", \"three\")` should raise a `TypeError` since both inputs are strings\n",
            "\n",
            "It is important to note that the `add` function does not perform any type checking or validation. Therefore, some of the scenarios listed above may not be handled correctly, resulting in unexpected behavior or errors.\u001b[0m\n",
            "[user]\n",
            "In addition to those scenarios above, list a few rare or unexpected edge cases (and as before, under each edge case, include a few examples as sub-bullets).\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "Here are some rare or unexpected edge cases for the `add` function:\n",
            "\n",
            "1. Addition with `None`:\n",
            "   - Example: `add(None, 5)` should raise a `TypeError` since one of the inputs is `None`\n",
            "   - Example: `add(10, None)` should raise a `TypeError` since one of the inputs is `None`\n",
            "   - Example: `add(None, None)` should raise a `TypeError` since both inputs are `None`\n",
            "\n",
            "2. Addition with `NaN` (Not a Number):\n",
            "   - Example: `add(float('nan'), 5)` should return `nan`\n",
            "   - Example: `add(10, float('nan'))` should return `nan`\n",
            "   - Example: `add(float('nan'), float('nan'))` should return `nan`\n",
            "\n",
            "3. Addition with `Infinity` and `-Infinity`:\n",
            "   - Example: `add(float('inf'), 5)` should return `inf`\n",
            "   - Example: `add(10, float('-inf'))` should return `-inf`\n",
            "   - Example: `add(float('inf'), float('-inf'))` should return `nan`\n",
            "\n",
            "4. Addition with complex numbers:\n",
            "   - Example: `add(2 + 3j, 4 + 5j)` should return `(6+8j)`\n",
            "   - Example: `add(1j, -1j)` should return `0j`\n",
            "   - Example: `add(2 + 3j, 4)` should return `(6+3j)`\n",
            "\n",
            "5. Addition with large floating-point numbers:\n",
            "   - Example: `add(1e100, 1e-100)` should return `1.0000000000000001e+100`\n",
            "   - Example: `add(1e-100, 1e100)` should return `1.0000000000000001e+100`\n",
            "   - Example: `add(1e100, -1e-100)` should return `9.999999999999999e+99`\n",
            "\n",
            "6. Addition with extremely large numbers:\n",
            "   - Example: `add(10**1000, 10**1000)` should raise an `OverflowError` due to exceeding the maximum float value\n",
            "\n",
            "These edge cases are less common and may not be explicitly handled in the `add` function. It is important to consider these scenarios to ensure the function behaves as expected and does not produce unexpected results or errors.\u001b[0m\n",
            "[system]\n",
            "You are a world-class Python developer with an eagle eye for unintended bugs and edge cases. You write careful, accurate unit tests. When asked to reply only with code, you write all of your code in a single block.\n",
            "\u001b[0m\n",
            "[user]\n",
            "Using Python and the `pytest` package, write a suite of unit tests for the function, following the cases above. Include helpful comments to explain each line. Reply only with code, formatted as follows:\n",
            "\n",
            "```python\n",
            "# imports\n",
            "import pytest  # used for our unit tests\n",
            "{insert other imports as needed}\n",
            "\n",
            "# function to test\n",
            "\n",
            "def add(a,b):\n",
            "  return a+b\n",
            "\n",
            "\n",
            "# unit tests\n",
            "# below, each test case is represented by a tuple passed to the @pytest.mark.parametrize decorator\n",
            "{insert unit test code here}\n",
            "```\n",
            "<generator object EngineAPIResource.create.<locals>.<genexpr> at 0x7d991026adc0>\n",
            "\u001b[92m\n",
            "[assistant]\n",
            "```python\n",
            "# imports\n",
            "import pytest\n",
            "\n",
            "# function to test\n",
            "def add(a, b):\n",
            "    return a + b\n",
            "\n",
            "# unit tests\n",
            "@pytest.mark.parametrize(\"a, b, expected\", [\n",
            "    # Basic addition\n",
            "    (2, 3, 5),\n",
            "    (0, 0, 0),\n",
            "    (-5, 8, 3),\n",
            "    # Addition with floating-point numbers\n",
            "    (2.5, 1.75, 4.25),\n",
            "    (0.1, 0.2, 0.3),\n",
            "    (-3.5, 6.25, 2.75),\n",
            "    # Addition with negative numbers\n",
            "    (-2, -3, -5),\n",
            "    (-5, 3, -2),\n",
            "    (-2.5, -1.75, -4.25),\n",
            "    # Addition with large numbers\n",
            "    (1000000, 2000000, 3000000),\n",
            "    (999999999, 1, 1000000000),\n",
            "    (1e100, 1e100, 2e100),\n",
            "    # Addition with different data types\n",
            "    (\"Hello\", \" World\", \"Hello World\"),\n",
            "    ([1, 2, 3], [4, 5, 6], [1, 2, 3, 4, 5, 6]),\n",
            "    ((1, 2), (3, 4), (1, 2, 3, 4)),\n",
            "])\n",
            "def test_add(a, b, expected):\n",
            "    assert add(a, b) == expected\n",
            "\n",
            "# Additional edge cases\n",
            "def test_add_with_none():\n",
            "    with pytest.raises(TypeError):\n",
            "        add(None, 5)\n",
            "    with pytest.raises(TypeError):\n",
            "        add(10, None)\n",
            "    with pytest.raises(TypeError):\n",
            "        add(None, None)\n",
            "\n",
            "def test_add_with_nan():\n",
            "    assert add(float('nan'), 5) == float('nan')\n",
            "    assert add(10, float('nan')) == float('nan')\n",
            "    assert add(float('nan'), float('nan')) == float('nan')\n",
            "\n",
            "def test_add_with_infinity():\n",
            "    assert add(float('inf'), 5) == float('inf')\n",
            "    assert add(10, float('-inf')) == float('-inf')\n",
            "    assert add(float('inf'), float('-inf')) == float('nan')\n",
            "\n",
            "def test_add_with_complex_numbers():\n",
            "    assert add(2 + 3j, 4 + 5j) == 6 + 8j\n",
            "    assert add(1j, -1j) == 0j\n",
            "    assert add(2 + 3j, 4) == 6 + 3j\n",
            "\n",
            "def test_add_with_large_floats():\n",
            "    assert add(1e100, 1e-100) == 1.0000000000000001e+100\n",
            "    assert add(1e-100, 1e100) == 1.0000000000000001e+100\n",
            "    assert add(1e100, -1e-100) == 9.999999999999999e+99\n",
            "\n",
            "def test_add_with_large_numbers():\n",
            "    with pytest.raises(OverflowError):\n",
            "        add(10**1000, 10**1000)\n",
            "```\n",
            "\n",
            "In the code above, we have defined a suite of unit tests for the `add` function using `pytest`. The `@pytest.mark.parametrize` decorator is used to specify the test cases and their expected results. Each test case is represented by a tuple of input values (`a` and `b`) and the expected output (`expected`). The `test_add` function is then defined to execute these test cases and assert that the actual output matches the expected output.\n",
            "\n",
            "Additionally, we have added separate test functions to cover the edge cases mentioned earlier. These test functions use `pytest.raises` to verify that the function raises the expected exceptions when given invalid inputs.\n",
            "\n",
            "By running `pytest` on this test suite, you can verify that the `add` function behaves correctly for a wide range of inputs and handles edge cases appropriately."
          ]
        }
      ],
      "source": [
        "example_function = \"\"\"\n",
        "def add(a,b):\n",
        "  return a+b\n",
        "\"\"\"\n",
        "\n",
        "unit_tests = unit_tests_from_function(\n",
        "    example_function,\n",
        "    approx_min_cases_to_cover=10,\n",
        "    print_text=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYLiNO3qmwhv",
        "outputId": "d8c9f610-d346-4ed5-996b-36dad32fd315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# imports\n",
            "import pytest\n",
            "\n",
            "# function to test\n",
            "def add(a, b):\n",
            "    return a + b\n",
            "\n",
            "# unit tests\n",
            "@pytest.mark.parametrize(\"a, b, expected\", [\n",
            "    # Basic addition\n",
            "    (2, 3, 5),\n",
            "    (0, 0, 0),\n",
            "    (-5, 8, 3),\n",
            "    # Addition with floating-point numbers\n",
            "    (2.5, 1.75, 4.25),\n",
            "    (0.1, 0.2, 0.3),\n",
            "    (-3.5, 6.25, 2.75),\n",
            "    # Addition with negative numbers\n",
            "    (-2, -3, -5),\n",
            "    (-5, 3, -2),\n",
            "    (-2.5, -1.75, -4.25),\n",
            "    # Addition with large numbers\n",
            "    (1000000, 2000000, 3000000),\n",
            "    (999999999, 1, 1000000000),\n",
            "    (1e100, 1e100, 2e100),\n",
            "    # Addition with different data types\n",
            "    (\"Hello\", \" World\", \"Hello World\"),\n",
            "    ([1, 2, 3], [4, 5, 6], [1, 2, 3, 4, 5, 6]),\n",
            "    ((1, 2), (3, 4), (1, 2, 3, 4)),\n",
            "])\n",
            "def test_add(a, b, expected):\n",
            "    assert add(a, b) == expected\n",
            "\n",
            "# Additional edge cases\n",
            "def test_add_with_none():\n",
            "    with pytest.raises(TypeError):\n",
            "        add(None, 5)\n",
            "    with pytest.raises(TypeError):\n",
            "        add(10, None)\n",
            "    with pytest.raises(TypeError):\n",
            "        add(None, None)\n",
            "\n",
            "def test_add_with_nan():\n",
            "    assert add(float('nan'), 5) == float('nan')\n",
            "    assert add(10, float('nan')) == float('nan')\n",
            "    assert add(float('nan'), float('nan')) == float('nan')\n",
            "\n",
            "def test_add_with_infinity():\n",
            "    assert add(float('inf'), 5) == float('inf')\n",
            "    assert add(10, float('-inf')) == float('-inf')\n",
            "    assert add(float('inf'), float('-inf')) == float('nan')\n",
            "\n",
            "def test_add_with_complex_numbers():\n",
            "    assert add(2 + 3j, 4 + 5j) == 6 + 8j\n",
            "    assert add(1j, -1j) == 0j\n",
            "    assert add(2 + 3j, 4) == 6 + 3j\n",
            "\n",
            "def test_add_with_large_floats():\n",
            "    assert add(1e100, 1e-100) == 1.0000000000000001e+100\n",
            "    assert add(1e-100, 1e100) == 1.0000000000000001e+100\n",
            "    assert add(1e100, -1e-100) == 9.999999999999999e+99\n",
            "\n",
            "def test_add_with_large_numbers():\n",
            "    with pytest.raises(OverflowError):\n",
            "        add(10**1000, 10**1000)\n"
          ]
        }
      ],
      "source": [
        "print(unit_tests)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_python_file(file_name, file_content):\n",
        "\n",
        "  with open(file_name, 'w') as file:\n",
        "        file.write(file_content)\n",
        "  return True"
      ],
      "metadata": {
        "id": "rZC9ZN1YL8aJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_python_file('test.py',unit_tests)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHADhrfZMLg-",
        "outputId": "4521271f-02da-4fe2-c22c-4946d1f5db1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgmLZWANmwhv"
      },
      "source": [
        "Make sure to check any code before using it, as GPT makes plenty of mistakes (especially on character-based tasks like this one). For best results, use the most powerful model (GPT-4, as of May 2023)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.9 ('openai')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}