{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tune LLMs using LLaMA Factory\n",
        "\n",
        "- https://github.com/hiyouga/LLaMA-Factory\n",
        "- Fine tune mistral 7b instruction version\n",
        "\n",
        "### Game plan\n",
        "- mistral > fine tune > Docker natural language commands dataset > RAG\n",
        "\n",
        "- Q: How to delete a docker container.\n",
        "- A: docker rm -f [container-name]\n",
        "\n",
        "### Datasets\n",
        " - Data on huggingface\n",
        "- https://huggingface.co/datasets/MattCoddity/dockerNLcommands\n",
        "\n",
        "- Use Gradio UI\n",
        "- https://www.gradio.app/"
      ],
      "metadata": {
        "id": "Eg-yMJA8mK7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup libraries"
      ],
      "metadata": {
        "id": "8v2i_6hmoAL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf65F1BEnj1O",
        "outputId": "be5e5342-51e5-4f53-823c-b8527c258232"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 23190, done.\u001b[K\n",
            "remote: Counting objects: 100% (189/189), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 23190 (delta 135), reused 115 (delta 115), pack-reused 23001 (from 2)\u001b[K\n",
            "Receiving objects: 100% (23190/23190), 46.49 MiB | 22.23 MiB/s, done.\n",
            "Resolving deltas: 100% (16891/16891), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqOtmvV4njyR",
        "outputId": "3b67374e-8474-4778-98ed-7a1c7321796a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0MHWf2YoPgA",
        "outputId": "bc4b1aae-792a-45c7-a118-0d1526232f1e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4yhWYiDoPdb",
        "outputId": "a4011041-65aa-45ef-bda2-addcfa47095c",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (4.49.0)\n",
            "Requirement already satisfied: datasets<=3.5.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.2.1)\n",
            "Collecting peft<=0.15.1,>=0.14.0 (from -r requirements.txt (line 4))\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.21.0)\n",
            "Requirement already satisfied: gradio<=5.25.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (5.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (5.29.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (0.34.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (2.3.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.26.4)\n",
            "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (2.10.6)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (2.2.2)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (14.3.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.8.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (0.30.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.11.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (15.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 13)) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements.txt (line 17)) (3.0.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->-r requirements.txt (line 18)) (4.9.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->-r requirements.txt (line 22)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->-r requirements.txt (line 22)) (2.27.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 23)) (2025.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 25)) (1.1.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (1.7.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->-r requirements.txt (line 2)) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 25)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 25)) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 16)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 25)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.6.0,>=0.34.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->-r requirements.txt (line 7)) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (0.1.2)\n",
            "Downloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.12.0\n",
            "    Uninstalling peft-0.12.0:\n",
            "      Successfully uninstalled peft-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llamafactory 0.9.2 requires peft<=0.12.0,>=0.11.1, but you have peft 0.15.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf2ZCxEtxF_V",
        "outputId": "ff9fbfa3-9851-4415-a1dd-b54f942e03a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install peft>=0.12.0,<=0.15.1"
      ],
      "metadata": {
        "id": "ytcVYG5fTfPd",
        "outputId": "95681bd9-bc4a-4339-987e-e7b376215760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: =0.15.1: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llamafactory"
      ],
      "metadata": {
        "collapsed": true,
        "id": "p1LEPwYRSkk9",
        "outputId": "a9b254c3-2c8a-44ff-98df-c3ffc779cb03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llamafactory in /usr/local/lib/python3.11/dist-packages (0.9.2)\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (4.49.0)\n",
            "Requirement already satisfied: datasets<=3.2.0,>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (3.2.0)\n",
            "Requirement already satisfied: accelerate<=1.2.1,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (1.2.1)\n",
            "Collecting peft<=0.12.0,>=0.11.1 (from llamafactory)\n",
            "  Using cached peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.9.6)\n",
            "Requirement already satisfied: tokenizers<=0.21.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.21.0)\n",
            "Requirement already satisfied: gradio<=5.21.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (5.21.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory) (5.29.4)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.34.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory) (2.10.6)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.115.12)\n",
            "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/dist-packages (from llamafactory) (2.3.3)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (3.10.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (1.26.4)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (from llamafactory) (14.3.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.11.0)\n",
            "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory) (0.8.14)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.2.1,>=0.34.0->llamafactory) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->llamafactory) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.2.0,>=2.16.0->llamafactory) (3.11.15)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.2 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (1.7.2)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (3.1.6)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (11.1.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.11.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory) (2.27.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->llamafactory) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory) (13.9.4)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory) (1.7.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory) (3.0.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->llamafactory) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory) (1.0.8)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.2.0,>=2.16.0->llamafactory) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate<=1.2.1,>=0.34.0->llamafactory) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory) (0.1.2)\n",
            "Using cached peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "Installing collected packages: peft\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.1\n",
            "    Uninstalling peft-0.15.1:\n",
            "      Successfully uninstalled peft-0.15.1\n",
            "Successfully installed peft-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!DISABLE_VERSION_CHECK=1"
      ],
      "metadata": {
        "id": "a920S5hcToge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changes in LLaMA Factory code"
      ],
      "metadata": {
        "id": "kVbtNz03uItg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ],
      "metadata": {
        "id": "4uU8_K-moPav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b023a9-081f-4f41-c80e-eb29e626849e",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-25 20:30:18.614456: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745613018.648537    3036 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745613018.659032    3036 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-25 20:30:18.691372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://18bd4582a409456fc3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "2025-04-25 20:34:30.861146: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745613270.881887    4126 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745613270.888442    4126 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-25 20:34:38] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "config.json: 100% 736/736 [00:00<00:00, 4.91MB/s]\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:34:38,404 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:34:38,405 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 237/237 [00:00<00:00, 1.49MB/s]\n",
            "vocab.json: 100% 798k/798k [00:00<00:00, 6.49MB/s]\n",
            "merges.txt: 100% 456k/456k [00:00<00:00, 1.94MB/s]\n",
            "tokenizer.json: 100% 2.11M/2.11M [00:00<00:00, 20.3MB/s]\n",
            "added_tokens.json: 100% 1.08k/1.08k [00:00<00:00, 7.86MB/s]\n",
            "special_tokens_map.json: 100% 99.0/99.0 [00:00<00:00, 640kB/s]\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:40,289 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:34:41,306 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:34:41,307 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:34:41,390 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-04-25 20:34:41] llamafactory.data.template:157 >> Add pad token: <|endoftext|>\n",
            "[INFO|2025-04-25 20:34:41] llamafactory.data.loader:157 >> Loading dataset alpaca_en_demo.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 1000 examples [00:00, 9601.64 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1699.40 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:04<00:00, 237.16 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[20490, 25, 39373, 4892, 257, 1429, 286, 1642, 1126, 12272, 13, 198, 48902, 25, 23874, 1126, 12272, 318, 281, 2562, 290, 12625, 1429, 0, 3423, 389, 2239, 12, 1525, 12, 9662, 7729, 319, 703, 284, 787, 606, 25, 198, 198, 16, 13, 1081, 15140, 534, 9391, 13, 1114, 4096, 1126, 12272, 11, 345, 1183, 761, 25, 352, 6508, 477, 12, 29983, 10601, 11, 362, 9653, 11, 352, 14, 17, 6508, 7545, 11, 352, 14, 17, 6508, 1660, 11, 352, 14, 19, 22326, 8268, 11, 290, 362, 30064, 24178, 9215, 13, 198, 198, 17, 13, 15561, 262, 4984, 25, 554, 257, 1588, 17090, 9396, 11, 21060, 1978, 262, 10601, 290, 262, 9653, 13, 17701, 935, 751, 262, 7545, 290, 1660, 11, 26547, 7558, 284, 4155, 326, 612, 389, 645, 300, 8142, 13, 3060, 8268, 290, 24178, 9215, 11, 290, 5022, 880, 13, 198, 198, 18, 13, 3914, 262, 4984, 1334, 25, 1002, 345, 460, 11, 1309, 262, 4984, 1650, 329, 281, 1711, 393, 523, 13, 770, 481, 1037, 262, 10601, 284, 17565, 262, 8122, 290, 787, 262, 1126, 12272, 517, 15403, 13, 198, 198, 19, 13, 12308, 534, 3425, 25, 3771, 25080, 257, 1729, 12, 13915, 3425, 625, 7090, 4894, 13, 4401, 306, 9215, 262, 3425, 393, 779, 10801, 11662, 284, 2948, 262, 1126, 12272, 422, 17274, 13, 198, 198, 20, 13, 39128, 262, 4984, 25, 8554, 257, 9717, 293, 393, 257, 15964, 6508, 11, 12797, 257, 1402, 2033, 286, 4984, 357, 10755, 352, 14, 19, 6508, 8, 4291, 262, 3641, 286, 262, 3425, 13, 34528, 26500, 262, 3425, 287, 257, 18620, 6268, 284, 4104, 262, 4984, 21894, 290, 44722, 625, 262, 4220, 286, 262, 3425, 13, 198, 198, 21, 13, 8261, 262, 1126, 431, 25, 8261, 262, 1126, 431, 329, 352, 12, 17, 2431, 1566, 262, 4220, 318, 15376, 10861, 13, 7276, 2759, 40953, 262, 13015, 351, 257, 15246, 4712, 290, 14283, 262, 1126, 431, 625, 284, 4255, 262, 584, 1735, 329, 1194, 5664, 13, 198, 198, 22, 13, 17220, 290, 9585, 25, 402, 1473, 10649, 262, 1126, 431, 4291, 257, 7480, 11, 290, 788, 9585, 262, 1429, 351, 262, 5637, 4984, 13, 11436, 284, 302, 12, 4360, 353, 262, 3425, 1022, 1123, 1126, 431, 611, 3306, 13, 198, 198, 23, 13, 27845, 290, 4691, 25, 27845, 534, 15847, 1126, 12272, 351, 534, 10348, 12591, 11, 884, 355, 4713, 8234, 11, 30712, 8566, 11, 11959, 12627, 11, 393, 8891, 290, 9891, 13, 8299, 393, 5591, 11, 290, 4691, 3393, 13, 18179, 0, 50256, 198]\n",
            "inputs:\n",
            "Human: Describe a process of making crepes.\n",
            "Assistant:Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<|endoftext|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 23874, 1126, 12272, 318, 281, 2562, 290, 12625, 1429, 0, 3423, 389, 2239, 12, 1525, 12, 9662, 7729, 319, 703, 284, 787, 606, 25, 198, 198, 16, 13, 1081, 15140, 534, 9391, 13, 1114, 4096, 1126, 12272, 11, 345, 1183, 761, 25, 352, 6508, 477, 12, 29983, 10601, 11, 362, 9653, 11, 352, 14, 17, 6508, 7545, 11, 352, 14, 17, 6508, 1660, 11, 352, 14, 19, 22326, 8268, 11, 290, 362, 30064, 24178, 9215, 13, 198, 198, 17, 13, 15561, 262, 4984, 25, 554, 257, 1588, 17090, 9396, 11, 21060, 1978, 262, 10601, 290, 262, 9653, 13, 17701, 935, 751, 262, 7545, 290, 1660, 11, 26547, 7558, 284, 4155, 326, 612, 389, 645, 300, 8142, 13, 3060, 8268, 290, 24178, 9215, 11, 290, 5022, 880, 13, 198, 198, 18, 13, 3914, 262, 4984, 1334, 25, 1002, 345, 460, 11, 1309, 262, 4984, 1650, 329, 281, 1711, 393, 523, 13, 770, 481, 1037, 262, 10601, 284, 17565, 262, 8122, 290, 787, 262, 1126, 12272, 517, 15403, 13, 198, 198, 19, 13, 12308, 534, 3425, 25, 3771, 25080, 257, 1729, 12, 13915, 3425, 625, 7090, 4894, 13, 4401, 306, 9215, 262, 3425, 393, 779, 10801, 11662, 284, 2948, 262, 1126, 12272, 422, 17274, 13, 198, 198, 20, 13, 39128, 262, 4984, 25, 8554, 257, 9717, 293, 393, 257, 15964, 6508, 11, 12797, 257, 1402, 2033, 286, 4984, 357, 10755, 352, 14, 19, 6508, 8, 4291, 262, 3641, 286, 262, 3425, 13, 34528, 26500, 262, 3425, 287, 257, 18620, 6268, 284, 4104, 262, 4984, 21894, 290, 44722, 625, 262, 4220, 286, 262, 3425, 13, 198, 198, 21, 13, 8261, 262, 1126, 431, 25, 8261, 262, 1126, 431, 329, 352, 12, 17, 2431, 1566, 262, 4220, 318, 15376, 10861, 13, 7276, 2759, 40953, 262, 13015, 351, 257, 15246, 4712, 290, 14283, 262, 1126, 431, 625, 284, 4255, 262, 584, 1735, 329, 1194, 5664, 13, 198, 198, 22, 13, 17220, 290, 9585, 25, 402, 1473, 10649, 262, 1126, 431, 4291, 257, 7480, 11, 290, 788, 9585, 262, 1429, 351, 262, 5637, 4984, 13, 11436, 284, 302, 12, 4360, 353, 262, 3425, 1022, 1123, 1126, 431, 611, 3306, 13, 198, 198, 23, 13, 27845, 290, 4691, 25, 27845, 534, 15847, 1126, 12272, 351, 534, 10348, 12591, 11, 884, 355, 4713, 8234, 11, 30712, 8566, 11, 11959, 12627, 11, 393, 8891, 290, 9891, 13, 8299, 393, 5591, 11, 290, 4691, 3393, 13, 18179, 0, 50256, 198]\n",
            "labels:\n",
            "Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:\n",
            "\n",
            "1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.\n",
            "\n",
            "2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.\n",
            "\n",
            "3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.\n",
            "\n",
            "4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.\n",
            "\n",
            "5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.\n",
            "\n",
            "6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.\n",
            "\n",
            "7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.\n",
            "\n",
            "8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy!<|endoftext|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:34:48,200 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:34:48,201 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 2.84G/2.84G [00:15<00:00, 178MB/s]\n",
            "[INFO|modeling_utils.py:3982] 2025-04-25 20:35:04,484 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/model.safetensors\n",
            "[INFO|modeling_utils.py:1633] 2025-04-25 20:35:04,519 >> Instantiating PhiForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:35:04,522 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:4970] 2025-04-25 20:35:05,925 >> All model checkpoint weights were used when initializing PhiForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4978] 2025-04-25 20:35:05,926 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 74.0/74.0 [00:00<00:00, 546kB/s]\n",
            "[INFO|configuration_utils.py:1095] 2025-04-25 20:35:06,190 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:35:06,191 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|2025-04-25 20:35:06] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-25 20:35:06] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-25 20:35:06] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-25 20:35:06] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-04-25 20:35:06] llamafactory.model.model_utils.misc:157 >> Found linear modules: k_proj,dense,fc2,v_proj,fc1,q_proj\n",
            "[INFO|2025-04-25 20:35:07] llamafactory.model.loader:157 >> trainable params: 7,077,888 || all params: 1,425,348,608 || trainable%: 0.4966\n",
            "[INFO|trainer.py:746] 2025-04-25 20:35:07,297 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:781] 2025-04-25 20:35:07,298 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|trainer.py:2405] 2025-04-25 20:35:07,701 >> ***** Running training *****\n",
            "[INFO|trainer.py:2406] 2025-04-25 20:35:07,701 >>   Num examples = 1,000\n",
            "[INFO|trainer.py:2407] 2025-04-25 20:35:07,701 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2408] 2025-04-25 20:35:07,701 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2411] 2025-04-25 20:35:07,701 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2412] 2025-04-25 20:35:07,701 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2413] 2025-04-25 20:35:07,701 >>   Total optimization steps = 186\n",
            "[INFO|trainer.py:2414] 2025-04-25 20:35:07,704 >>   Number of trainable parameters = 7,077,888\n",
            "  3% 5/186 [00:18<11:11,  3.71s/it][INFO|2025-04-25 20:35:26] llamafactory.train.callbacks:157 >> {'loss': 1.2927, 'learning_rate': 1.9964e-04, 'epoch': 0.08, 'throughput': 1058.71}\n",
            "{'loss': 1.2927, 'grad_norm': 0.15088137984275818, 'learning_rate': 0.0001996436098130433, 'epoch': 0.08, 'num_input_tokens_seen': 19808}\n",
            "  5% 10/186 [00:36<10:39,  3.63s/it][INFO|2025-04-25 20:35:44] llamafactory.train.callbacks:157 >> {'loss': 1.2422, 'learning_rate': 1.9858e-04, 'epoch': 0.16, 'throughput': 1091.78}\n",
            "{'loss': 1.2422, 'grad_norm': 0.19938229024410248, 'learning_rate': 0.00019857697953148037, 'epoch': 0.16, 'num_input_tokens_seen': 40224}\n",
            "  8% 15/186 [00:55<10:06,  3.54s/it][INFO|2025-04-25 20:36:02] llamafactory.train.callbacks:157 >> {'loss': 1.2413, 'learning_rate': 1.9681e-04, 'epoch': 0.24, 'throughput': 1118.75}\n",
            "{'loss': 1.2413, 'grad_norm': 0.27860134840011597, 'learning_rate': 0.00019680771188662044, 'epoch': 0.24, 'num_input_tokens_seen': 61648}\n",
            " 11% 20/186 [01:13<09:46,  3.53s/it][INFO|2025-04-25 20:36:20] llamafactory.train.callbacks:157 >> {'loss': 1.2616, 'learning_rate': 1.9435e-04, 'epoch': 0.32, 'throughput': 1098.37}\n",
            "{'loss': 1.2616, 'grad_norm': 0.26041844487190247, 'learning_rate': 0.00019434841787099803, 'epoch': 0.32, 'num_input_tokens_seen': 80256}\n",
            " 13% 25/186 [01:33<11:01,  4.11s/it][INFO|2025-04-25 20:36:41] llamafactory.train.callbacks:157 >> {'loss': 1.1089, 'learning_rate': 1.9122e-04, 'epoch': 0.40, 'throughput': 1114.97}\n",
            "{'loss': 1.1089, 'grad_norm': 0.2209373116493225, 'learning_rate': 0.00019121662684969335, 'epoch': 0.4, 'num_input_tokens_seen': 104656}\n",
            " 16% 30/186 [01:52<09:33,  3.68s/it][INFO|2025-04-25 20:37:00] llamafactory.train.callbacks:157 >> {'loss': 1.0468, 'learning_rate': 1.8743e-04, 'epoch': 0.48, 'throughput': 1113.36}\n",
            "{'loss': 1.0468, 'grad_norm': 0.2532016634941101, 'learning_rate': 0.00018743466161445823, 'epoch': 0.48, 'num_input_tokens_seen': 125376}\n",
            " 19% 35/186 [02:13<09:58,  3.96s/it][INFO|2025-04-25 20:37:21] llamafactory.train.callbacks:157 >> {'loss': 1.1192, 'learning_rate': 1.8303e-04, 'epoch': 0.56, 'throughput': 1117.28}\n",
            "{'loss': 1.1192, 'grad_norm': 0.19666029512882233, 'learning_rate': 0.00018302947927123766, 'epoch': 0.56, 'num_input_tokens_seen': 149040}\n",
            " 22% 40/186 [02:31<09:06,  3.74s/it][INFO|2025-04-25 20:37:38] llamafactory.train.callbacks:157 >> {'loss': 1.1632, 'learning_rate': 1.7803e-04, 'epoch': 0.64, 'throughput': 1112.44}\n",
            "{'loss': 1.1632, 'grad_norm': 0.15390023589134216, 'learning_rate': 0.0001780324790952092, 'epoch': 0.64, 'num_input_tokens_seen': 168016}\n",
            " 24% 45/186 [02:49<08:08,  3.47s/it][INFO|2025-04-25 20:37:56] llamafactory.train.callbacks:157 >> {'loss': 1.1437, 'learning_rate': 1.7248e-04, 'epoch': 0.72, 'throughput': 1111.87}\n",
            "{'loss': 1.1437, 'grad_norm': 0.20260632038116455, 'learning_rate': 0.000172479278722912, 'epoch': 0.72, 'num_input_tokens_seen': 187968}\n",
            " 27% 50/186 [03:05<07:25,  3.28s/it][INFO|2025-04-25 20:38:12] llamafactory.train.callbacks:157 >> {'loss': 1.1218, 'learning_rate': 1.6641e-04, 'epoch': 0.80, 'throughput': 1106.42}\n",
            "{'loss': 1.1218, 'grad_norm': 0.24190717935562134, 'learning_rate': 0.00016640946027672392, 'epoch': 0.8, 'num_input_tokens_seen': 204816}\n",
            " 30% 55/186 [03:24<08:20,  3.82s/it][INFO|2025-04-25 20:38:32] llamafactory.train.callbacks:157 >> {'loss': 1.0757, 'learning_rate': 1.5987e-04, 'epoch': 0.88, 'throughput': 1109.78}\n",
            "{'loss': 1.0757, 'grad_norm': 0.19498051702976227, 'learning_rate': 0.0001598662882312615, 'epoch': 0.88, 'num_input_tokens_seen': 227040}\n",
            " 32% 60/186 [03:40<06:51,  3.27s/it][INFO|2025-04-25 20:38:48] llamafactory.train.callbacks:157 >> {'loss': 1.1811, 'learning_rate': 1.5290e-04, 'epoch': 0.96, 'throughput': 1105.72}\n",
            "{'loss': 1.1811, 'grad_norm': 0.23396489024162292, 'learning_rate': 0.00015289640103269625, 'epoch': 0.96, 'num_input_tokens_seen': 243792}\n",
            " 35% 65/186 [03:57<06:57,  3.45s/it][INFO|2025-04-25 20:39:05] llamafactory.train.callbacks:157 >> {'loss': 1.1100, 'learning_rate': 1.4555e-04, 'epoch': 1.03, 'throughput': 1104.16}\n",
            "{'loss': 1.11, 'grad_norm': 0.19779853522777557, 'learning_rate': 0.0001455494786690634, 'epoch': 1.03, 'num_input_tokens_seen': 262080}\n",
            " 38% 70/186 [04:15<07:15,  3.75s/it][INFO|2025-04-25 20:39:23] llamafactory.train.callbacks:157 >> {'loss': 1.0467, 'learning_rate': 1.3788e-04, 'epoch': 1.11, 'throughput': 1099.60}\n",
            "{'loss': 1.0467, 'grad_norm': 0.1842840313911438, 'learning_rate': 0.0001378778885610576, 'epoch': 1.11, 'num_input_tokens_seen': 280976}\n",
            " 40% 75/186 [04:33<06:58,  3.77s/it][INFO|2025-04-25 20:39:41] llamafactory.train.callbacks:157 >> {'loss': 1.0743, 'learning_rate': 1.2994e-04, 'epoch': 1.19, 'throughput': 1098.23}\n",
            "{'loss': 1.0743, 'grad_norm': 0.2069796621799469, 'learning_rate': 0.00012993631229733582, 'epoch': 1.19, 'num_input_tokens_seen': 300912}\n",
            " 43% 80/186 [04:52<06:43,  3.80s/it][INFO|2025-04-25 20:40:00] llamafactory.train.callbacks:157 >> {'loss': 1.1069, 'learning_rate': 1.2178e-04, 'epoch': 1.27, 'throughput': 1099.55}\n",
            "{'loss': 1.1069, 'grad_norm': 0.2535165846347809, 'learning_rate': 0.00012178135587488515, 'epoch': 1.27, 'num_input_tokens_seen': 321712}\n",
            " 46% 85/186 [05:11<06:19,  3.76s/it][INFO|2025-04-25 20:40:19] llamafactory.train.callbacks:157 >> {'loss': 1.0610, 'learning_rate': 1.1347e-04, 'epoch': 1.35, 'throughput': 1093.67}\n",
            "{'loss': 1.061, 'grad_norm': 0.24422010779380798, 'learning_rate': 0.00011347114622258612, 'epoch': 1.35, 'num_input_tokens_seen': 341200}\n",
            " 48% 90/186 [05:32<06:44,  4.21s/it][INFO|2025-04-25 20:40:39] llamafactory.train.callbacks:157 >> {'loss': 1.0522, 'learning_rate': 1.0506e-04, 'epoch': 1.43, 'throughput': 1093.46}\n",
            "{'loss': 1.0522, 'grad_norm': 0.19426904618740082, 'learning_rate': 0.00010506491688387127, 'epoch': 1.43, 'num_input_tokens_seen': 363072}\n",
            " 51% 95/186 [05:50<05:59,  3.95s/it][INFO|2025-04-25 20:40:58] llamafactory.train.callbacks:157 >> {'loss': 1.0038, 'learning_rate': 9.6623e-05, 'epoch': 1.51, 'throughput': 1091.03}\n",
            "{'loss': 1.0038, 'grad_norm': 0.2819446325302124, 'learning_rate': 9.662258581165319e-05, 'epoch': 1.51, 'num_input_tokens_seen': 382944}\n",
            " 54% 100/186 [06:07<05:01,  3.51s/it][INFO|2025-04-25 20:41:15] llamafactory.train.callbacks:157 >> {'loss': 1.1079, 'learning_rate': 8.8204e-05, 'epoch': 1.59, 'throughput': 1088.29}\n",
            "{'loss': 1.1079, 'grad_norm': 0.3496707081794739, 'learning_rate': 8.820432828491542e-05, 'epoch': 1.59, 'num_input_tokens_seen': 400160}\n",
            " 54% 100/186 [06:07<05:01,  3.51s/it][INFO|trainer.py:3942] 2025-04-25 20:41:15,404 >> Saving model checkpoint to saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-100\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:41:15,637 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:41:15,639 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-04-25 20:41:15,720 >> tokenizer config file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-04-25 20:41:15,720 >> Special tokens file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-100/special_tokens_map.json\n",
            " 56% 105/186 [06:27<04:57,  3.68s/it][INFO|2025-04-25 20:41:34] llamafactory.train.callbacks:157 >> {'loss': 1.0361, 'learning_rate': 7.9870e-05, 'epoch': 1.67, 'throughput': 1085.13}\n",
            "{'loss': 1.0361, 'grad_norm': 0.3469519019126892, 'learning_rate': 7.987014799113397e-05, 'epoch': 1.67, 'num_input_tokens_seen': 420224}\n",
            " 59% 110/186 [06:48<05:24,  4.27s/it][INFO|2025-04-25 20:41:56] llamafactory.train.callbacks:157 >> {'loss': 1.1238, 'learning_rate': 7.1679e-05, 'epoch': 1.75, 'throughput': 1085.33}\n",
            "{'loss': 1.1238, 'grad_norm': 0.2938605546951294, 'learning_rate': 7.16794493317696e-05, 'epoch': 1.75, 'num_input_tokens_seen': 443696}\n",
            " 62% 115/186 [07:07<04:42,  3.98s/it][INFO|2025-04-25 20:42:15] llamafactory.train.callbacks:157 >> {'loss': 1.1009, 'learning_rate': 6.3691e-05, 'epoch': 1.83, 'throughput': 1086.13}\n",
            "{'loss': 1.1009, 'grad_norm': 0.2804558277130127, 'learning_rate': 6.369061399935255e-05, 'epoch': 1.83, 'num_input_tokens_seen': 464512}\n",
            " 65% 120/186 [07:28<04:25,  4.02s/it][INFO|2025-04-25 20:42:35] llamafactory.train.callbacks:157 >> {'loss': 1.0250, 'learning_rate': 5.5961e-05, 'epoch': 1.91, 'throughput': 1085.61}\n",
            "{'loss': 1.025, 'grad_norm': 0.31367871165275574, 'learning_rate': 5.596058484423656e-05, 'epoch': 1.91, 'num_input_tokens_seen': 486480}\n",
            " 67% 125/186 [07:45<03:38,  3.59s/it][INFO|2025-04-25 20:42:53] llamafactory.train.callbacks:157 >> {'loss': 1.0583, 'learning_rate': 4.8544e-05, 'epoch': 1.99, 'throughput': 1083.61}\n",
            "{'loss': 1.0583, 'grad_norm': 0.36913028359413147, 'learning_rate': 4.854445999713715e-05, 'epoch': 1.99, 'num_input_tokens_seen': 504752}\n",
            " 70% 130/186 [08:04<03:27,  3.71s/it][INFO|2025-04-25 20:43:11] llamafactory.train.callbacks:157 >> {'loss': 1.0957, 'learning_rate': 4.1495e-05, 'epoch': 2.06, 'throughput': 1083.22}\n",
            "{'loss': 1.0957, 'grad_norm': 0.34826526045799255, 'learning_rate': 4.149510014046922e-05, 'epoch': 2.06, 'num_input_tokens_seen': 524384}\n",
            " 73% 135/186 [08:22<03:08,  3.69s/it][INFO|2025-04-25 20:43:30] llamafactory.train.callbacks:157 >> {'loss': 1.0103, 'learning_rate': 3.4863e-05, 'epoch': 2.14, 'throughput': 1083.45}\n",
            "{'loss': 1.0103, 'grad_norm': 0.35942158102989197, 'learning_rate': 3.4862751727777797e-05, 'epoch': 2.14, 'num_input_tokens_seen': 544528}\n",
            " 75% 140/186 [08:39<02:45,  3.59s/it][INFO|2025-04-25 20:43:47] llamafactory.train.callbacks:157 >> {'loss': 1.0991, 'learning_rate': 2.8695e-05, 'epoch': 2.22, 'throughput': 1081.17}\n",
            "{'loss': 1.0991, 'grad_norm': 0.30249789357185364, 'learning_rate': 2.869468883687798e-05, 'epoch': 2.22, 'num_input_tokens_seen': 561792}\n",
            " 78% 145/186 [08:58<02:40,  3.91s/it][INFO|2025-04-25 20:44:06] llamafactory.train.callbacks:157 >> {'loss': 1.0167, 'learning_rate': 2.3035e-05, 'epoch': 2.30, 'throughput': 1082.52}\n",
            "{'loss': 1.0167, 'grad_norm': 0.2702421247959137, 'learning_rate': 2.3034876209506772e-05, 'epoch': 2.3, 'num_input_tokens_seen': 583280}\n",
            " 81% 150/186 [09:18<02:21,  3.94s/it][INFO|2025-04-25 20:44:26] llamafactory.train.callbacks:157 >> {'loss': 0.9732, 'learning_rate': 1.7924e-05, 'epoch': 2.38, 'throughput': 1082.01}\n",
            "{'loss': 0.9732, 'grad_norm': 0.2866022288799286, 'learning_rate': 1.7923655879272393e-05, 'epoch': 2.38, 'num_input_tokens_seen': 604240}\n",
            " 83% 155/186 [09:36<01:56,  3.76s/it][INFO|2025-04-25 20:44:44] llamafactory.train.callbacks:157 >> {'loss': 0.9888, 'learning_rate': 1.3397e-05, 'epoch': 2.46, 'throughput': 1081.95}\n",
            "{'loss': 0.9888, 'grad_norm': 0.3581533432006836, 'learning_rate': 1.339745962155613e-05, 'epoch': 2.46, 'num_input_tokens_seen': 623904}\n",
            " 86% 160/186 [09:57<01:43,  3.96s/it][INFO|2025-04-25 20:45:04] llamafactory.train.callbacks:157 >> {'loss': 1.0584, 'learning_rate': 9.4885e-06, 'epoch': 2.54, 'throughput': 1082.26}\n",
            "{'loss': 1.0584, 'grad_norm': 0.3563443422317505, 'learning_rate': 9.488549274967872e-06, 'epoch': 2.54, 'num_input_tokens_seen': 646416}\n",
            " 89% 165/186 [10:14<01:12,  3.46s/it][INFO|2025-04-25 20:45:21] llamafactory.train.callbacks:157 >> {'loss': 1.0219, 'learning_rate': 6.2248e-06, 'epoch': 2.62, 'throughput': 1081.15}\n",
            "{'loss': 1.0219, 'grad_norm': 0.38454896211624146, 'learning_rate': 6.22478678529197e-06, 'epoch': 2.62, 'num_input_tokens_seen': 663920}\n",
            " 91% 170/186 [10:34<01:03,  3.98s/it][INFO|2025-04-25 20:45:41] llamafactory.train.callbacks:157 >> {'loss': 1.0168, 'learning_rate': 3.6294e-06, 'epoch': 2.70, 'throughput': 1080.96}\n",
            "{'loss': 1.0168, 'grad_norm': 0.3384806215763092, 'learning_rate': 3.6294356110059157e-06, 'epoch': 2.7, 'num_input_tokens_seen': 685632}\n",
            " 94% 175/186 [10:52<00:40,  3.71s/it][INFO|2025-04-25 20:46:00] llamafactory.train.callbacks:157 >> {'loss': 1.0356, 'learning_rate': 1.7210e-06, 'epoch': 2.78, 'throughput': 1081.10}\n",
            "{'loss': 1.0356, 'grad_norm': 0.3765474557876587, 'learning_rate': 1.7209949059142083e-06, 'epoch': 2.78, 'num_input_tokens_seen': 705488}\n",
            " 97% 180/186 [11:09<00:21,  3.57s/it][INFO|2025-04-25 20:46:17] llamafactory.train.callbacks:157 >> {'loss': 1.0034, 'learning_rate': 5.1307e-07, 'epoch': 2.86, 'throughput': 1079.77}\n",
            "{'loss': 1.0034, 'grad_norm': 0.35796231031417847, 'learning_rate': 5.130676608104845e-07, 'epoch': 2.86, 'num_input_tokens_seen': 723184}\n",
            " 99% 185/186 [11:31<00:04,  4.08s/it][INFO|2025-04-25 20:46:38] llamafactory.train.callbacks:157 >> {'loss': 1.0179, 'learning_rate': 1.4264e-08, 'epoch': 2.94, 'throughput': 1080.53}\n",
            "{'loss': 1.0179, 'grad_norm': 0.33362075686454773, 'learning_rate': 1.426374402901942e-08, 'epoch': 2.94, 'num_input_tokens_seen': 746928}\n",
            "100% 186/186 [11:35<00:00,  4.03s/it][INFO|trainer.py:3942] 2025-04-25 20:46:42,886 >> Saving model checkpoint to saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-186\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:46:43,083 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:46:43,084 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-04-25 20:46:43,180 >> tokenizer config file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-186/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-04-25 20:46:43,181 >> Special tokens file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/checkpoint-186/special_tokens_map.json\n",
            "[INFO|trainer.py:2657] 2025-04-25 20:46:43,363 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 695.659, 'train_samples_per_second': 4.312, 'train_steps_per_second': 0.267, 'train_loss': 1.0870414747986743, 'epoch': 2.96, 'num_input_tokens_seen': 751440}\n",
            "100% 186/186 [11:35<00:00,  3.74s/it]\n",
            "[INFO|trainer.py:3942] 2025-04-25 20:46:43,365 >> Saving model checkpoint to saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:46:43,548 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:46:43,549 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2500] 2025-04-25 20:46:43,617 >> tokenizer config file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2509] 2025-04-25 20:46:43,618 >> Special tokens file saved in saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.96\n",
            "  num_input_tokens_seen    =     751440\n",
            "  total_flos               =  5544739GF\n",
            "  train_loss               =      1.087\n",
            "  train_runtime            = 0:11:35.65\n",
            "  train_samples_per_second =      4.312\n",
            "  train_steps_per_second   =      0.267\n",
            "Figure saved at: saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25/training_loss.png\n",
            "[WARNING|2025-04-25 20:46:43] llamafactory.extras.ploting:162 >> No metric eval_loss to plot.\n",
            "[WARNING|2025-04-25 20:46:43] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\n",
            "[INFO|modelcard.py:449] 2025-04-25 20:46:43,890 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:41,527 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:41,529 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:41,660 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:42,169 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:42,170 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:42,253 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-04-25 20:48:42] llamafactory.data.template:157 >> Add pad token: <|endoftext|>\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:42,470 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:42,471 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|2025-04-25 20:48:42] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3982] 2025-04-25 20:48:42,516 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/model.safetensors\n",
            "[INFO|modeling_utils.py:1633] 2025-04-25 20:48:42,531 >> Instantiating PhiForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:48:42,533 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:4970] 2025-04-25 20:48:44,035 >> All model checkpoint weights were used when initializing PhiForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4978] 2025-04-25 20:48:44,035 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-04-25 20:48:44,288 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:48:44,288 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|2025-04-25 20:48:44] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-25 20:48:44] llamafactory.model.loader:157 >> all params: 1,418,270,720\n",
            "[WARNING|2025-04-25 20:48:44] llamafactory.chat.hf_engine:168 >> There is no current event loop, creating a new one.\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:55,982 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:55,983 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,066 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:56,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:56,604 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2050] 2025-04-25 20:48:56,713 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-04-25 20:48:56] llamafactory.data.template:157 >> Add pad token: <|endoftext|>\n",
            "[INFO|configuration_utils.py:699] 2025-04-25 20:48:57,009 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-25 20:48:57,010 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": null,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": null,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"partial_rotary_factor\": 0.5,\n",
            "  \"qk_layernorm\": false,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "[INFO|2025-04-25 20:48:57] llamafactory.model.patcher:157 >> Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3982] 2025-04-25 20:48:57,012 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/model.safetensors\n",
            "[INFO|modeling_utils.py:1633] 2025-04-25 20:48:57,034 >> Instantiating PhiForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:48:57,036 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:4970] 2025-04-25 20:48:58,638 >> All model checkpoint weights were used when initializing PhiForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4978] 2025-04-25 20:48:58,639 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1095] 2025-04-25 20:48:58,841 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/675aa382d814580b22651a30acb1a585d7c25963/generation_config.json\n",
            "[INFO|configuration_utils.py:1140] 2025-04-25 20:48:58,842 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|2025-04-25 20:48:58] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-25 20:48:59] llamafactory.model.adapter:157 >> Merged 1 adapter(s).\n",
            "[INFO|2025-04-25 20:48:59] llamafactory.model.adapter:157 >> Loaded adapter(s): saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25\n",
            "[INFO|2025-04-25 20:48:59] llamafactory.model.loader:157 >> all params: 1,418,270,720\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://18bd4582a409456fc3.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "EXV3MMhBoPYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dbeae4-2fd8-405e-8dec-e931465ff00a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "The token `jarvis` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `jarvis`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli upload RickyRubini/my-phi-tuned /content/LLaMA-Factory/saves/Phi-1.5-1.3B/lora/train_2025-04-25-20-31-25"
      ],
      "metadata": {
        "id": "9S0fA_1m4C3f",
        "outputId": "ed07d1d0-1ad1-4474-f84d-1ef61a8f206e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consider using `hf_transfer` for faster uploads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
            "It seems you are trying to upload a large folder at once. This might take some time and then fail if the folder is too large. For such cases, it is recommended to upload in smaller batches or to use `HfApi().upload_large_folder(...)`/`huggingface-cli upload-large-folder` instead. For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\n",
            "Start hashing 48 files.\n",
            "Finished hashing 48 files.\n",
            "adapter_model.safetensors:   0% 0.00/28.3M [00:00<?, ?B/s]\n",
            "adapter_model.safetensors:   0% 0.00/28.3M [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "optimizer.pt:   0% 0.00/56.9M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rng_state.pth:   0% 0.00/14.2k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "Upload 14 LFS files:   0% 0/14 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scaler.pt:   0% 0.00/988 [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "scaler.pt: 100% 988/988 [00:00<00:00, 6.50kB/s]\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 76.3kB/s]\n",
            "adapter_model.safetensors:  25% 7.16M/28.3M [00:00<00:00, 26.6MB/s]\n",
            "adapter_model.safetensors:  39% 11.1M/28.3M [00:00<00:00, 36.0MB/s]\u001b[A\n",
            "\n",
            "adapter_model.safetensors:  36% 10.3M/28.3M [00:00<00:00, 28.4MB/s]\n",
            "\n",
            "\n",
            "scheduler.pt:   0% 0.00/1.06k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  56% 16.0M/28.3M [00:00<00:00, 38.4MB/s]\n",
            "\n",
            "training_args.bin: 100% 5.69k/5.69k [00:00<00:00, 40.5kB/s]\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 6.32kB/s]\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:   0% 0.00/28.3M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  70% 19.9M/28.3M [00:00<00:00, 23.1MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   0% 0.00/56.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  28% 16.0M/56.9M [00:00<00:02, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  10% 2.93M/28.3M [00:00<00:01, 19.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "adapter_model.safetensors:  84% 23.9M/28.3M [00:00<00:00, 27.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  94% 26.6M/28.3M [00:01<00:00, 24.4MB/s]\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  18% 5.00M/28.3M [00:00<00:01, 15.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  39% 22.3M/56.9M [00:01<00:01, 22.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   3% 1.75M/56.9M [00:00<00:08, 6.66MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  36% 10.2M/28.3M [00:00<00:00, 26.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  47% 26.9M/56.9M [00:01<00:01, 25.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 28.3M/28.3M [00:01<00:00, 21.8MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:   9% 5.24M/56.9M [00:00<00:04, 12.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 28.3M/28.3M [00:01<00:00, 19.1MB/s]\n",
            "rng_state.pth:   0% 0.00/14.2k [00:00<?, ?B/s]\n",
            "\n",
            "optimizer.pt:  56% 32.0M/56.9M [00:01<00:01, 18.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "rng_state.pth: 100% 14.2k/14.2k [00:00<00:00, 120kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  28% 16.0M/56.9M [00:00<00:01, 22.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  68% 38.6M/56.9M [00:01<00:00, 23.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "adapter_model.safetensors:  78% 22.1M/28.3M [00:01<00:00, 22.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scaler.pt:   0% 0.00/988 [00:00<?, ?B/s]\n",
            "\n",
            "optimizer.pt:  75% 42.9M/56.9M [00:01<00:00, 24.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "scaler.pt: 100% 988/988 [00:00<00:00, 7.87kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "scheduler.pt:   0% 0.00/1.06k [00:00<?, ?B/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  56% 32.0M/56.9M [00:01<00:00, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "scheduler.pt: 100% 1.06k/1.06k [00:00<00:00, 9.42kB/s]\n",
            "training_args.bin: 100% 5.69k/5.69k [00:00<00:00, 57.7kB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "adapter_model.safetensors: 100% 28.3M/28.3M [00:01<00:00, 17.0MB/s]\n",
            "training_args.bin: 100% 5.69k/5.69k [00:00<00:00, 57.8kB/s]\n",
            "\n",
            "\n",
            "optimizer.pt:  84% 48.0M/56.9M [00:02<00:00, 17.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  75% 42.8M/56.9M [00:01<00:00, 32.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "optimizer.pt:  98% 56.0M/56.9M [00:02<00:00, 24.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  82% 46.5M/56.9M [00:01<00:00, 31.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt:  88% 50.0M/56.9M [00:02<00:00, 19.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt: 100% 56.9M/56.9M [00:03<00:00, 18.5MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "optimizer.pt: 100% 56.9M/56.9M [00:02<00:00, 21.2MB/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Upload 14 LFS files: 100% 14/14 [00:03<00:00,  3.78it/s]\n",
            "https://huggingface.co/RickyRubini/my-phi-tuned/tree/main/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load tokenizer and base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-1_5\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-1_5\")\n",
        "model = PeftModel.from_pretrained(base_model, \"RickyRubini/my-phi-tuned\")\n",
        "\n",
        "# Define input prompt\n",
        "query_to_llm = \"What is the recipe for making crepes?\"\n",
        "\n",
        "# Tokenize and generate\n",
        "inputs = tokenizer.encode(query_to_llm, return_tensors=\"pt\")\n",
        "outputs = model.generate(\n",
        "    inputs,\n",
        "    max_new_tokens=150\n",
        ")\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "6ghKaCoF5pRE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29d40507-779c-479a-e1db-6ea54cd4a5fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "id": "MEin7oczv9RM",
        "outputId": "f201d2c1-6d30-423e-cb50-d027971856d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24446,   502,   262,  1407, 15544,   329,  1642,   257,  1126, 12272,\n",
              "            13,   198,   198,    32,    25,   464,  1407, 15544,   329,  1642,\n",
              "           257,  1126, 12272,   318,   262,  9391,   290,   262,  1429,   286]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fu9A57spv-8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}